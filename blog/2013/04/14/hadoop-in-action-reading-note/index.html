
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!-->
<html class="no-js" lang="zh-CN"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Hadoop in Action学习笔记 - Zhou&#8217;s Blog</title>
  <meta name="author" content="yoyzhou">
  <!--[if lt IE 9]>
    <script src="http://x.papaapp.com/farm1/a571d2/8dda131d/html5shiv.js"></script>
    <![endif]-->
  
  <meta name="description" content="Hadoop实践（Hadoop in Action）读书笔记">
  <meta name="keywords" content="Hadoop, MapReduce, 读书笔记, Bigdata, 大数据, ReadingNote, Hive, HBase, Pig">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://yoyzhou.github.io/blog/2013/04/14/hadoop-in-action-reading-note/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script type="text/javascript" src="http://t.papaapp.com/js/libs/jquery/1.7.2/jquery.js"></script>
   <link href="/stylesheets/data-table.css" media="screen, projection" rel="stylesheet" type="text/css" />
  <script src="/javascripts/modernizr-2.0.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
	      jax: ["input/TeX", "output/HTML-CSS"],
		  tex2jax: {
		  inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
		  displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
		  processEscapes: true,
		  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
		}}
   );
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
<script src="/javascripts/ender.js"></script>
<script src="/javascripts/octopress.js" type="text/javascript"></script>
<link href="/atom.xml" rel="alternate" title="Zhou's Blog" type="application/atom+xml">

  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-34034666-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header id="header" class="clearfix">    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                <a id="logo" href="/">
                   Zhou&#8217;s Blog
                </a>
                <p class="description">There is no truth as such, there is only truth from a point of view [Nietzsche]</p>
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="index-nav current" href="/">Blog</a>
<a class="archives-nav" href="/blog/archives">Archives</a>
                </nav>
            </div>
        </div>
    </div>
</header>
  <div id="body">
    <div class="container">
    	<div class="col-group">
			<div class="col-8" id="main">
  <div class="res-cons">
  <article class="post clearfix">
  
  <header>
    
      <h1 class="post-title">Hadoop in Action学习笔记</h1>
    
    
      <p class="post-meta">
        








  


<time datetime="2013-04-14T15:41:00-07:00" pubdate data-updated="true"></time>
        
         | <a href="#disqus_thread">Comments</a>
        
      </p>
    
  </header>


<div class="post-content"><h4 id="hadoop">第一章 Hadoop简介</h4>

<p>现今，互联网每天都产生海量的数据，现有工具对于TB、PB级别大规模分布式海量数据变得无力处理。</p>

<p>Google首先推出了处理大规模分布式数据的MapReduce计算范式，Doug Cutting领导开发了一个开源版的MapReduce，后来成为Hadoop。</p>

<h5 id="hadoop-1">什么是Hadoop</h5>

<p>Hadoop是一个开源框架，可编写和运行分布式应用处理大规模数据。一般认为Hadoop包含两个核心部分：HDFS，Hadoop分布式文件系统和MapReduce，一种分布式编程范式（分布式存储和分布式计算）。</p>

<pre><code>**优点：**
• 方便
• 健壮
• 线性可扩展
• 简单
</code></pre>

<h5 id="vs-">分布式系统  VS 大型单机服务器</h5>

<pre><code>向外扩展 VS  向上扩展

构建在一般/低端商用机器上，廉价

高端服务器，价格昂贵
</code></pre>

<h5 id="hadoopsetihome">Hadoop设计理念和SETI@Home的区别</h5>

<p>Hadoop设计用于数据密集型任务，遵循将程序向数据移动的设计哲学，即尽可能的保持数据不动，减少I/O的访问，程序文件的数量级相对于数据的数量级小得多</p>

<p>SETI@Home设计用于计算密集型任务，遵循将数据向计算资源（计算机）移动的设计哲学；因为数据传输量小，但是需要大量的计算资源（CPU时间）</p>

<h5 id="hadoopsql">Hadoop与SQL数据库的比较</h5>
<p>都是处理数据，SQL数据库用于处理结构化数据</p>

<p>Hadoop应用针对的是文本数据/可能是结构的也可能使非结构或者半结构化数据</p>

<p>1. 向外扩展 代替向上扩展</p>

<p>2. 用键/值对 代替关系表</p>

<p>3. 用函数式编程（MapReduce）代替申明式查询（SQL）</p>

<p>4. 用离线处理代替在线处理</p>

<p>数据密集型分布式应用中，数据传输的代价昂贵，应保持数据存储和处理紧密的绑定在一起。</p>

<h5 id="hadoop-2">Hadoop的历史</h5>
<p>2004年 Goole发表Google文件系统（GFS）和MapReduce框架</p>

<p>Doug Cutting将Nutch项目移植到GFS和MapReduce上，并设立了专门的项目充实这两种技术，于是就有了Hadoop</p>

<p>2006年 Yahoo聘用Doug Cutting，让他和一个专门团队一起改进Hadoop</p>

<p>2008年 Hahoop成为Apache的顶级项目</p>

<h4 id="hadoop-3">第二章 初识Hadoop</h4>

<p><strong>“运行Hadoop”意味着在不同服务器运行一组守护进程（daemons）:</strong></p>

<pre><code>0 NameNode (名字节点)
1 DataNode (数据节点)
2 Secondary NameNode (次名字节点)
3 JobTracker (作业跟踪节点)
4 TaskTracker (任务跟踪节点)
</code></pre>

<p>Hadoop在分布式计算和分布式存储中都采用了<strong>主/从(Master/Slaver)</strong>的结构</p>

<p>NameNode，JobTracker是Master节点</p>

<p>DataNode，TaskTracker和Slaver节点</p>

<h5 id="hadoop-4">运行Hadoop</h5>
<p>配置文件：core-site.xml, hdfs-site.xml和mapred-site.xml</p>

<p>conf/masters - master节点主机列表</p>

<p>conf/slaves - slave节点主机列表</p>

<p>运行的三种模式：本地（单机）模式，伪分布模式和全分布模式</p>

<pre><code>bin/hadoop namenode -format
bin/start-all.sh
jps
</code></pre>

<h5 id="web">基于WEB的管理界面</h5>
<p>NameNode状态界面： namenode-host:50070</p>

<p>JobTracker状态界面： jobtacker-host:50030</p>

<h4 id="hadoop-5">第三章 Hadoop组件</h4>

<p><strong>HDFS文件系统</strong></p>

<p>HDFS是一种文件系统，专为MapReduce这类框架下的大规模分布式处理而设计的</p>

<p>可以处理单个的大数据集（比如100TB），而大多数文件系统物理实现这点。</p>

<p><strong>？</strong>是不是说HDFS处理单个大数据集更加有效，而处理小文件的大量数据变现的不是很有优势，当然，如果这样也可以先把小文件组成大文件。</p>

<p><strong>？</strong>文件要多大才能适合HDFS/Hadoop的处理呢?</p>

<p>可不要为了数据大而大，数据应该在必要的清洗和预处理</p>

<h5 id="hadoop-6">Hadoop的基本文件命令</h5>

<p>hadoop fs -cmd &lt;args&gt;</p>

<p>HDFS中，默认的根目录是/user/$USER 其中USER是你登录系统的用户名</p>

<p>1. 添加文件和目录</p>

<p><code>hadoop fs -mkdir &lt;dir-name&gt;</code></p>

<p>将在/user/$USER目录下创建&lt;dir-name&gt;，如果&lt;dir-name&gt;包含多层目录且上层目录不存在，hadoop fs -mkdir会创建指定的多层目录结构。</p>

<p>如:</p>

<p><code>hadoop fs -mkdir inputs/dataset1 会创建/user/$USER/inputs/dataset1</code></p>

<p>2. 列举文件清单</p>

<p><code>hadoop fs -ls &lt;file-dir-name&gt;</code></p>

<p>和Unix系统ls本地文件类似</p>

<p><code>hadoop fs -lsr &lt;file-dir-name&gt;</code></p>

<p>查看所有文件和文件的子目录，递归的列出文件清单</p>

<p>3. 复制本地文件到HDFS </p>

<p><code>hadoop fs -put &lt;file-dir-name-to-put-in-hdfs&gt; &lt;file-dir-on-hdfs&gt;</code></p>

<p>4. 从HDFS上检索文件到本地系统</p>

<p><code>hadoop fs -get &lt;file-dir-on-hdfs-to-get&gt; &lt;file-dir-name-on-localhost&gt;</code></p>

<p>这是一个与hadoop fs -put相反的操作。</p>

<p>5. 查看HDFS上文件的内容</p>

<p><code>hadoop fs -cat &lt;file-name-to-cat&gt;</code></p>

<p>例如：</p>

<p><code>hadoop fs -cat example.txt</code></p>

<p>同样假设文件在默认的工作目录/user/$USER下</p>

<p>6. 删除文件/目录</p>

<p><code>hadoop fs -rm &lt;file-name-to-remove&gt;</code></p>

<p>例如：</p>

<p><code>hadoop fs -rm example.txt</code></p>

<p>这个命令用来删除文件，要删除文件夹是使用 -rmr 命令：</p>

<p><code>hadoop fs -rmr &lt;dir-name-to-remove&gt;</code></p>

<p>例如：</p>

<p><code>hadoop fs -rmr input</code></p>

<p>这里的<code>r</code>代表递归的删除文件。</p>

<p>7. 查看文件命令帮助</p>

<p><code>hadoop fs -help &lt;cmd&gt;</code></p>

<p>例如：</p>

<p><code>hadoop fs -help rmr</code></p>

<p>查看rmr命令的帮助文件</p>

<p>8. hadoop文件命令与Unix管道一起使用</p>

<p>例如：</p>

<p><code>hadoop fs -cat example.txt | head -n 20</code></p>

<h5 id="hadoop-7">Hadoop数据的读和写</h5>

<p>遵循并行处理数据分片原则的输入数据通常为单一的大文件。这也是Hadoop分布式文件系统的设计策略。</p>

<p>FSDataInputStream支持随机读取，这一特性涉及到MapReduce的数据分片机制。</p>

<p>随机读取，当一个任务失败时，恢复时不用从头读取文件，只需要从失败的位置进行恢复。</p>

<p><strong>InputFormat</strong></p>

<p>Hadoop分割与读取输入文件的方式在InputFormat接口中定义，TextInputFormat是InputFormat的默认实现。Hadoop提供的其他的InputFormat实现有：</p>

<p>KeyValueTextInputFormat，SequenceFileInputFormat和NLineInputFormat</p>

<p><strong>OutputFormat</strong></p>

<p>通过使用IntWritable类变量可以提高reduce()的性能，IntWritable类的处理性能要比Text高。</p>

<h4 id="mapreduce">第四章 编写MapReduce基础程序</h4>

<p><strong>mapper container partitioner reducer</strong> </p>

<h5 id="combiner">使用Combiner提升性能</h5>

<p>Combiner作用上与Reduce等价，起到聚合、结合作用，原则上程序必须没有Combiner也能够得到正确的结果</p>

<p>1. 网络洗牌时减少数据传输流量，考虑10亿条Mapper键值对的输出，如果没有Combiner会在网络洗牌过程中造成多大的流量</p>

<p>2. 数据分布不均匀时，造成大量Mapper的键值对输出跑向同一个Reducer</p>

<p>原理就是通过Combiner减少Mapper的输出数量以降低网络和Reducer上的压力，Combiner位于Mapper和Reducer中间，被视为是Reducer的助手，在数据转换上必须与Reducer等价，也就是如果我们去掉Combiner，Reducer的输出应该保持不变。</p>

<p>但是Combiner未必会提高性能，这要看Combiner是否能有效的减少Mapper输出记录的数量。</p>

<h4 id="mapreduce-1">第五章 高阶MapReduce</h4>

<h5 id="mapreduce-2">MapReduce之间的依赖</h5>

<p>Hadoop通过Job和JobControl类来管理作业之间的非线性依赖关系。</p>

<p>x.addDependingJob(y)</p>

<p>ChainMapper ChainReducer</p>

<p>链接MapReduce是，mapper、reducer之间的输入输出按照值传递还是引用传递的问题。P100</p>

<p>如果确保上游的Map/Reduce不是用其输出数据，或者下游Map/Reduce不改变上游的输出数据，可以使用by reference以提高一定的性能</p>

<h5 id="section">联结不同来源的数据</h5>

<p>1. Reduce侧的联结</p>

<p>repartitioned join 重分区联结</p>

<p>repartitioned sort-mergejoin 重分区排序-合并联结</p>

<p>结合DataJoin包使用钩子处理数据流P93</p>

<p>2. Mapper侧的联结P98</p>

<p><strong>DistributedCache 分布式缓存</strong></p>

<p>应用场景：一个数据源较大，另一个数据源可能小几个数量级，将较小的数据源装入内存，复制到所有的Mapper，在map阶段执行联结。通常小的数据源也叫做“背景数据”</p>

<h4 id="section-1">第六章 编程实践</h4>

<h5 id="section-2">本地模式</h5>

<p><strong>1 计算的完整性检查</strong></p>

<p>数学和逻辑错误在数据密集型程序中更加普遍，而它们往往并不明显！</p>

<p>数学计数或者算术如何检查正确性，在分布式计算中数学公式可能要重新设计，但是如何验证计算的完整性和准确性也就十分的重要。</p>

<p><strong>方法：</strong></p>

<p>可是通过宏观的查看一些指标，比如平均值，整体计数、最大值等来进行初步验证</p>

<p><strong>2 回归测试</strong></p>

<p>可能代价会很大，但是可以选取一部分数据集进行测试。所谓回归测试就是在同一数据集上比较代码修改前后算法的输出结果是否一致，如果每次回归测试的结果都是一样的，那么可以判定修改没有导致出现新的问题。</p>

<p><strong>3 考虑使用LONG而不是INT</strong></p>

<h5 id="section-3">伪分布模式</h5>
<p>1 日志</p>

<p>2 JOBTRACKER的WEB管理界面</p>

<p>3 杀掉作业</p>

<h5 id="section-4">生产集群上的监控与调试</h5>
<p>计数器 Reporter.incrCounter()</p>

<p>跳过坏记录 skipping模式的设置P126</p>

<p>用IsolationRunner重新运行出错的任务</p>

<h5 id="section-5">性能调优</h5>

<p><strong>Hadoop的线性可扩展性</strong></p>

<h6 id="combiner-1">1 通过combiner来减少网络流量</h6>

<h6 id="section-6">2 减少输入数据</h6>
<pre><code>a. 采样数据，只处理数据的子集
b. “重构”数据，仅使用需要的字段（要求开发人员清楚的了解数据和自己的业务需求）
#上面两个方案一个是**横切**一个是**纵切**，都是减少输入数据的方法
</code></pre>

<h6 id="section-7">3 使用压缩</h6>

<p>即使使用了combiner，在map阶段的输出也可能很大。这些中间数据必须被存储在磁盘上，并在网络上重排。压缩这些中间数据会提高大多数MapReduce作业的性能。Hadoop内置支持压缩和解压缩。</p>

<pre><code>压缩的参数配置P130
mapred.compress.map.output
mapred.map.output.compression.codec
SequenceFIleOutputFormat 序列文件输出
</code></pre>

<h6 id="jvm">4 重用JVM</h6>

<p>mapred.job.reuse.jvm.num.task 指定一个JVM可以运行的最大任务数</p>

<h6 id="section-8">5 根据猜测执行来运行</h6>

<p>在所有的mapper完成之前，reducer都不会启动；类似的，在所有的reducer完成之前，一个作业也不会结束。</p>

<p><strong>问题：当其中一个任务变慢时（注意不是失效），MapReduce如何处理？</strong></p>

<p>Hadoop会注意到运行速度缓慢的任务，并安排在另一个节点上并行执行相同的任务。</p>

<pre><code>**参数：**

mapred.map.tasks.speculative.execution
mapred.reduce.tasks.speculative.execution
</code></pre>

<h6 id="section-9">6 代码重构与算法重写</h6>

<p>6.1 将Streaming程序重写为Java程序</p>

<p>6.2 在一次作业中集中一次处理类似的任务，而不是每一个任务都启动一个作业，比如计算最大值、最小值和均值等在同一数据集上的计算可以在一个Job中完成，而不要分为不同的Job</p>

<p>6.3 设计特定的新的适合MapReduce框架的算法</p>

<h4 id="section-10">第七章 细则手册</h4>

<h5 id="section-11">7.1 向任务传递作业定制的参数</h5>

<p>在JobConf中定制自己的参数，在所有的Task中都能读取到。</p>

<h5 id="section-12">7.2 探查任务特定信息</h5>

<h5 id="section-13">7.3 划分为多个输出文件</h5>
<p>MultipleOutputFormat 键/值区分，写入不同的文件， 一个Collector 不同文件名</p>

<p>MultipleOutputs	属性区分，写入不同的文件，多个Collector写文件</p>

<h5 id="section-14">7.4 以数据库作为输入输出</h5>
<p><strong>DBOutputFormat</strong></p>

<h5 id="section-15">7.5 保持输出的顺序</h5>

<h4 id="hadoop-8">第八章 管理Hadoop</h4>

<h5 id="section-16">8.1 为实际应用设置特定参数值</h5>

<h5 id="section-17">8.2 系统体检</h5>
<pre><code>bin/hadoop fsck &lt;path&gt; #file system check
bin/hadoop dfsadmin -report
</code></pre>

<h5 id="section-18">8.3 权限设置</h5>

<h5 id="section-19">8.4 配额管理</h5>
<pre><code>bin/hadoop dfsadmin -setQuota &lt;N&gt; dir
bin/hadoop dfsadmin -setSpaceQuota &lt;N&gt; dir
bin/hadoop fs -count -q dir #显示目录的配额
</code></pre>

<h5 id="section-20">8.5 启动回收站</h5>

<p>fs.trash.interval属性（以分钟为单位）</p>

<h5 id="datanode">8.6 删减DataNode</h5>

<pre><code>dfs.hosts.exclude=file-point-to-exclude-host-list
bin/hadoop dfsadmin -refreshNodes
</code></pre>

<h5 id="datanode-1">8.7 增加DataNode</h5>

<h5 id="namenodesnn">8.8 管理NameNode和SNN</h5>
<p>减轻NameNode负担的一种方法是增加数据块的大小，以降低文件系统中元数据的数据量，dfs.block.size 默认64M</p>

<h6 id="snn">SNN是做什么的？</h6>
<p>取了一个不妥的名字。首先并不是NameNode的失效备份</p>

<p>把SNN视为一个检查点服务器更为合适，用于合并下面两个文件以形成系统快照。</p>

<pre><code>可能在0.21中被弃用
FsImage
EditLog
</code></pre>

<h5 id="namenode">8.9 恢复失效的NameNode</h5>
<p>备份NameNode的元数据文件（dfs.name.dir）到SNN节点，或者其他多个节点。</p>

<h5 id="section-21">8.10 感知网络布局和机架的设计</h5>
<p><strong>DataNode数据块的副本策略</strong>
标准副本为3个：</p>

<p>第1个 在同一个DataNode上</p>

<p>第2个 不同的机架上的DataNode上</p>

<p>第3个 与第二个同机架的不同DataNode上</p>

<p>如果大于3个副本，其他的随机放置的不同节点上</p>

<p>topology.script.file.name</p>

<p>机架感知，配置Hadoop是NameNode知道DataNode的机架分布结构</p>

<h5 id="section-22">8.11 多用户作业调度</h5>
<p>8.11.1 多个JobTracker</p>

<p>8.11.2 公平调度器 （Facebook）</p>

<h4 id="hadoop-9">第九章 在云上运行Hadoop</h4>
<p><strong>租用 经济</strong>
Amazon Web Services(AWS)</p>

<p>Elastic Compute Clous(EC2) 弹性计算云</p>

<p>Simple Storage Service(S3)简单存储服务</p>

<p>1 AWS与外部网络通信的带宽需要收取费用，EC2内部不需要收费。</p>

<p>2 先存储在S3 在从S3中复制到主节点</p>

<p>S3中存储数据同样要收费 但是 存储空间可扩展、一次存储可以多次使用、资费相对低、S3和EC2是内部网，之间复制数据没有费用且速度快</p>

<p>3 还可以直接将S3作为输入输出文件地址，而不用复制数据到HDFS</p>

<h4 id="pignot-finished">第十章 用Pig编程（Not Finished）</h4>
<p>Pig是架构在Hadoop之上的高级数据处理层。</p>

<p><strong>易用性、高性能、大规模可扩展能力</strong>是所有Hadoop子项目的期望</p>

<p><strong>“Pig吃任何东西”</strong></p>

<p>Pig Latin命令运行： Grunt Shell、脚本文件、嵌入在Java程序中。</p>

<h4 id="hivehadoop">第十一章 Hive及Hadoop群</h4>
<p>Pig——一种高级数据流语言</p>

<p>Hive——一种类SQL数据仓库基础设施</p>

<p>HBase——一种模仿Google BigTable的分布式的、面向列的数据库</p>

<p>ZooKeeper——一种用于管理分布式应用之间共享状态的可靠的协同系统</p>

<p>Cascading——是Hadoop上用于组装和执行复杂数据处理工作流的一个API</p>

<p>Hama——矩阵计算软件包，用于计算乘积、逆、特征值、特征向量和其他矩阵运算</p>

<p>Mahout——基于Hadoop实现机器学习算法</p>

<h5 id="hive">Hive</h5>
<p>Hive是建立在Haddop基础之上的数据仓库软件包</p>

<p>HiveQL——类SQL的数据查询语言</p>

<p>MetaStore ——用于存放元数据的组件，存储在Derby关系数据库中（存取速度的考虑）</p>

<p><code>---EOF---</code></p>

</div>


  <footer class="post-footer">
    <div class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">yoyzhou</span></span>
 under 

<span class="categories">
  
    <a class='category' href='/blog/categories/bigdata/'>bigdata</a>, <a class='category' href='/blog/categories/hadoop/'>hadoop</a>, <a class='category' href='/blog/categories/mapreduce/'>mapreduce</a>, <a class='category' href='/blog/categories/du-shu-bi-ji/'>读书笔记</a>
  
</span>


    </div>
    
      <div class="sharing">
  
  
  
</div>

    
    <div class="meta">
      
        <a class="basic-alignment left" href="/blog/2013/04/08/weibo-scrapy-framework-with-multi-threading/" title="Previous Post: 基于UID的WEIBO信息抓取框架WEIBO_SCRAPY">&laquo; 基于UID的WEIBO信息抓取框架WEIBO_SCRAPY</a>
      
      
        <a class="basic-alignment right" href="/blog/2013/04/14/steps-of-programming-hadoop-with-eclipse/" title="Next Post: 使用Eclipse开发MapReduce程序的步骤">使用Eclipse开发MapReduce程序的步骤 &raquo;</a>
      
    </div>
  </footer>
</article>

  <section>
    <h3>Comments</h3>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

  </div>
</div>

  <aside id="secondary">
  
    
<section class="widget">
	<h3 class="widget-title">About Me</h3>
	<p><code style="font-size: 14px;">Writing is the best way of learning</code></p>
</section>
<section class="widget">
	<h3 class="widget-title">Recent Posts</h3>
	<ul class="widget-list">
		
     	<li>
      	  <a href="/blog/2013/06/04/mahout-clustering-with-hadoop/">在Hadoop上运行Mahout KMeans聚类分析</a>
      	</li>
    	
     	<li>
      	  <a href="/blog/2013/05/26/clustering-with-mahout/">Mahout与聚类分析</a>
      	</li>
    	
     	<li>
      	  <a href="/blog/2013/05/13/hadoop-write-ur-own-rawcomparator/">使用RawComparator加速Hadoop程序</a>
      	</li>
    	
     	<li>
      	  <a href="/blog/2013/05/10/hadoop-serialization-and-writable-object-2/">Hadoop序列化与Writable接口(二)</a>
      	</li>
    	
     	<li>
      	  <a href="/blog/2013/05/09/hadoop-serialization-and-writable-object-1/">Hadoop序列化与Writable接口(一)</a>
      	</li>
    	
     	<li>
      	  <a href="/blog/2013/04/29/viz-following-networks-of-weibo-celebrities/">微博名人关注网络的社会网络分析</a>
      	</li>
    	
     	<li>
      	  <a href="/blog/2013/04/29/a-kinda-betweenness-centrality-algorithm/">A Kinda Betweenness Centrality Algorithm</a>
      	</li>
    	
     	<li>
      	  <a href="/blog/2013/04/28/studying-notes-a-set-of-measures-of-centrality-based-on-betweenness/">A Set of Measures of Centrality Based on Betweenness</a>
      	</li>
    	
     	<li>
      	  <a href="/blog/2013/04/21/adding-filter-in-hadoop-mapper-class/">Adding Filter in Hadoop Mapper Class</a>
      	</li>
    	
     	<li>
      	  <a href="/blog/2013/04/14/steps-of-programming-hadoop-with-eclipse/">使用Eclipse开发MapReduce程序的步骤</a>
      	</li>
    	
	</ul>
</section>

<section class="widget">
  <h3>GitHub Repos</h3>
  <ul id="gh_repos" class="widget-list">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/yoyzhou">@yoyzhou</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'yoyzhou',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>


      	</div>
    </div>
  </div>
  <footer id="footer">
  	<div class="container">
	Copyright &copy; 2014 - yoyzhou -
  <span class="credit">Powered by <a rel="nofollow" href="http://octopress.org">Octopress</a> on <a rel="nofollow" href="http://pages.github.com/">GitHubPages</a>
  </span>
  - <span class="credit">Theme by <a rel="nofollow" href="https://github.com/yoyzhou/mewpassant">mewpassant</a></span>


</div>

  </footer>
  

<script type="text/javascript">
      var disqus_shortname = 'yoyzhou';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://yoyzhou.github.io/blog/2013/04/14/hadoop-in-action-reading-note/';
        var disqus_url = 'http://yoyzhou.github.io/blog/2013/04/14/hadoop-in-action-reading-note/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
